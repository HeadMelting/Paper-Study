{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from helper_plot import hdr_plot_style\n",
    "\n",
    "device = 'mps'\n",
    "hdr_plot_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(size, noise=0.5):\n",
    "    x, _ = make_swiss_roll(size, noise=noise)\n",
    "    return x[:, [0, 2]] / 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sample_batch(10 ** 4).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Sliced Score Matching\n",
    "# Jacobian vector product trick\n",
    "def sliced_score_matching(model, samples):\n",
    "    samples.requires_grad_(True)\n",
    "\n",
    "    # Construct random vectors\n",
    "    vectors = torch.randn_like(samples)\n",
    "    vectors = vectors / torch.norm(vectors, dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute the optimized vector-product jacobian\n",
    "    logp, jvp = torch.autograd.functional.jvp(model, samples, vectors, create_graph=True)\n",
    "\n",
    "    # Compute the norm loss\n",
    "    norm_loss = (logp * vectors) ** 2 / 2.\n",
    "\n",
    "    # Compute the Jacobian loss\n",
    "    v_jvp = jvp * vectors\n",
    "    jacob_loss = v_jvp\n",
    "    loss = jacob_loss + norm_loss\n",
    "    return loss.mean(-1).mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoising_score_mathcing(scorenet, samples, sigma=0.01):\n",
    "    perturbed_samples = samples + torch.randn_like(samples) * sigma\n",
    "    target = -1 / (sigma ** 2) * (perturbed_samples - samples)\n",
    "    scores = scorenet(perturbed_samples)\n",
    "    target = target.view(target.shape[0], -1)\n",
    "    scores = scores.view(scores.shape[0], -1)\n",
    "    \n",
    "    loss = 1 / 2 * ((scores - target) ** 2).sum(dim=-1).mean(dim=0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 128), nn.Softplus(),\n",
    "    nn.Linear(128, 128), nn.Softplus(),\n",
    "    nn.Linear(128, 2)\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "dataset = torch.tensor(data.T).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for t in range(5000):\n",
    "        print(f'\\r{t}/5000', end='   ')\n",
    "        loss = denoising_score_mathcing(model, dataset)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not t % 1000: print(loss)\n",
    "\n",
    "# train()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradients(model, data, plot_scatter=True):\n",
    "    xx = np.stack(np.meshgrid(np.linspace(-1.5, 2.0, 50), np.linspace(-1.5, 2.0, 50)), axis=-1).reshape(-1, 2)\n",
    "    scores = model(torch.from_numpy(xx).float()).detach().cpu()\n",
    "    scores_norm = np.linalg.norm(scores, axis=-1, ord=2, keepdims=True)\n",
    "    scores_log1p = scores / (scores_norm + 1e-9) * np.log1p(scores_norm)\n",
    "\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    if plot_scatter:\n",
    "        plt.scatter(*data, alpha=0.3, color='red', edgecolors='white', s=40)\n",
    "    plt.quiver(xx.T[0], xx.T[1], scores_log1p[:,0], scores_log1p[:,1], width=0.002, color='white')\n",
    "    plt.xlim(-1.5, 2.0)\n",
    "    plt.ylim(-1.5, 2.0)\n",
    "\n",
    "# plot_gradients(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_langevin(model, x, n_steps=10, eps=1e-3, decay=.9, temperature=1.0):\n",
    "    x_sequence = [x.unsqueeze(0)]\n",
    "    for _ in range(n_steps):\n",
    "        z_t = torch.randn(x.size())\n",
    "        x = x + (eps / 2) * model(x) + (np.sqrt(eps) * temperature * z_t)\n",
    "        x_sequence.append(x.unsqueeze(0))\n",
    "        eps *= decay\n",
    "    return torch.cat(x_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_langevin(x=None):\n",
    "    if x is None:\n",
    "        x = torch.Tensor([1.5, -1.5])\n",
    "    samples = sample_langevin(model, x).detach()\n",
    "\n",
    "    plot_gradients(model, data)\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], color='green', edgecolors='white', s=150)\n",
    "    deltas = (samples[1:] - samples[:-1])\n",
    "    deltas = deltas - deltas / torch.tensor(np.linalg.norm(deltas, keepdims=True, axis=-1)) * 0.04\n",
    "    for i, arrow in enumerate(deltas):\n",
    "        plt.arrow(samples[i, 0], samples[i, 1], arrow[0], arrow[1], width=1e-4, head_width=2e-2, color='green', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siren",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
